---
title: 深度学习的数学原理-复杂函数求导的链式传递及多变量近似公式
tags:
  - 复合函数求导
  - 机器学习
  - 梯度下降
  - 近似公式
id: '535'
categories:
  - - 机器学习
date: 2020-02-28 14:06:55
cover: /static/uploads/2020/02/zWPNti_1543055279031.jpeg
---



## 前言

前文中对导数、偏导数已经有了概念并能进行简单计算，本篇主要介绍单变量和多变量的复合函数如何求导，以及近似公式的计算

## 正文

### 复杂函数求导

如果一个函数比较复杂，无论是用公式也好，还是用导数的定义强算也好，都比较麻烦，于是书中又介绍了关于复合函数的快速求导方法（高中数学知识），以及扩展了多变量复合函数的链式法则。

#### 神经网络中的复合函数

这里又祭出那个十分熟悉的公式 [![](/static/uploads/2020/02/d82257e59778f41c417cce7cc8f2467b.png)](/static/uploads/2020/02/d82257e59778f41c417cce7cc8f2467b.png)

> w1, w2, …, wn为各输入对应的权重，b为神经单元的偏置。输出函数是如下的x1, x2, …, xn的一次函数f和激活函数a的复合函数。

可以把这个函数看为两个函数的复合函数 [![](/static/uploads/2020/02/56817e2d494d4ea7ce55454a4d223fff.png)](/static/uploads/2020/02/56817e2d494d4ea7ce55454a4d223fff.png)

#### 单变量复合函数链式法则

当已知 `y = f(u)` `u = g(x)`，其导数可以依照下方所示的公式进行计算 [![](/static/uploads/2020/02/fba40b6051139354f5d4e505a1ae187b.png)](/static/uploads/2020/02/fba40b6051139354f5d4e505a1ae187b.png) 仅看公式可能看不太明白，可以看一下书中的例题，例题中的符号有点难打，就直接贴图了。 [![](/static/uploads/2020/02/4e9c77057da4ab5c2b73634c9fb81270.png)](/static/uploads/2020/02/4e9c77057da4ab5c2b73634c9fb81270.png) [![](/static/uploads/2020/02/458b99389277462a23f87e656fa616b8.png)](/static/uploads/2020/02/458b99389277462a23f87e656fa616b8.png)

#### 多变量函数链式法则

多变量函数的偏导数求导，也有一个链式法则。 比如变量**z为u、v的函数**，如果**u、v分别为x、y的函数**，则z为x、y的函数，**z关于x求导**时，先对**u、v求导**，然后**与z的相应导数相乘**，最后将乘积加起来。 [![](/static/uploads/2020/02/b4602851640d4c47fe5b846f7d1b3c85.png)](/static/uploads/2020/02/b4602851640d4c47fe5b846f7d1b3c85.png)

### 函数的近似公式

个人看下来，觉着作用是减少计算机计算量，书中强调这是梯度下降法的基础；

#### 单变量函数的近似公式

根据导数公式，Δx->0 的时候，解出来的式子就是其一阶导数，如果Δ的值很小，那么下面的式子是成立的 [![](/static/uploads/2020/02/72e85f5edc1109b13c15d3adfe7cd23d.png)](/static/uploads/2020/02/72e85f5edc1109b13c15d3adfe7cd23d.png) 此时，将 f(x+Δx) 转换出来，就成了下面这个样子，这个式子就叫做**单变量函数的近似公式** [![](/static/uploads/2020/02/2c94ffba3bb552b2bfd6815ae00a2068.png)](/static/uploads/2020/02/2c94ffba3bb552b2bfd6815ae00a2068.png) 那么这个式子有什么用处呢？通过一个栗子来看一下

> 当f(x)=ex时，求x=0附近的近似公式。

根据公式可以得到如下的结果： [![](/static/uploads/2020/02/976bd20640884f6a69202151ccdcef22.png)](/static/uploads/2020/02/976bd20640884f6a69202151ccdcef22.png) 取 x = 0，再将Δx换成X，可得到 `e^x = 1+x` 在图形中有这样的含义，意思是 x=0 的时候 e^x 与 1+x 的结果相似： [![](/static/uploads/2020/02/aa9c59a2c191e5acff21d9dde0f5f95e.png)](/static/uploads/2020/02/aa9c59a2c191e5acff21d9dde0f5f95e.png)

#### 多变量函数的近似公式

这个公式可能比较好理解，但是不大好推导，x和y分别变化一个较小的数，其结果与 f(x, y) + x的偏导数_Δx + y的偏导数_Δy 近似相同 [![](/static/uploads/2020/02/f51db77b8ec0078a49f26c3755a5e747.png)](/static/uploads/2020/02/f51db77b8ec0078a49f26c3755a5e747.png) 可以通过书中的一个例子来理解这个公式

> 举个例子：当z=e^(x+y)时，求x=y=0附近的近似公式

按照公式，不难得出 `f(x+Δx, y+Δy) = e^(x+y) + e^(x+y)*Δx + e^(x+y)*Δy = e^(x+y)*(1+Δx+Δy)` 带入 x+y = 0，并将 Δx 换为 x ，Δy 换成 y 即可 `1 + x + y` 书中还提到了简化多变量函数近似公式的方法，首先有个 Δz 定义为x和y同时进行一个很小的变化与原函数值的差值 [![](/static/uploads/2020/02/8d48d500dfd6c487706b0546526d4e64.png)](/static/uploads/2020/02/8d48d500dfd6c487706b0546526d4e64.png) 那么根据公式以及上述定义进行一下变换，可以得到 [![](/static/uploads/2020/02/7c9d6e385d5d4af379fb86184dcab242.png)](/static/uploads/2020/02/7c9d6e385d5d4af379fb86184dcab242.png) 再对变量数量进行推广 [![](/static/uploads/2020/02/92f52441bc4f316d7f91217e34468534.png)](/static/uploads/2020/02/92f52441bc4f316d7f91217e34468534.png) 有没有很眼熟，是不是很像向量的内积，所以书中又提到了这个公式可以用向量内积的形式体现 [![](/static/uploads/2020/02/2b304a537c0dbcb4ebd57c56d670cede.png)](/static/uploads/2020/02/2b304a537c0dbcb4ebd57c56d670cede.png)

## 总结

主要介绍了单变量和多变量复合函数求导，还有单变量和多变量的近似公式及其向量内积表示，为日后梯度下降的学习打下基础。