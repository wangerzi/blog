---
title: 深度学习的数学-卷积神经网络的误差反向传播
tags:
  - BP算法
  - 卷积神经网络
  - 反向递推关系式
  - 深度学习
id: '581'
categories:
  - - 机器学习
date: 2020-03-12 18:38:30
cover: /static/uploads/2020/02/zWPNti_1543055279031.jpeg
---



## 前言

本篇博客主要记录反向传播法在卷积神经网络中的应用，如何像全连接的隐藏层那样，通过输出层的神经单元误差反向推算出所有层的神经单元误差，最终得到梯度。

## 正文

博主在看完卷积神经网络的正向运作流程后，其实是有一点懵圈的，于是我冷静了一天再继续看卷积神经网络的反向传递；正向运作流程中的 『池化』、『特征映射』应该怎么样用式子表示？经过了池化（最大/平均/L2）的神经单元输入，又怎样体现在神经单元误差的计算上？可以带着这些问题继续往下看。

### 卷积神经网络中的关系式

在进行推算之前，首先确定好每个层神经单元之间的关系式 [![](/static/uploads/2020/03/baa9475f78f8ac9a3fcf407980013972.png)](/static/uploads/2020/03/baa9475f78f8ac9a3fcf407980013972.png)

#### 卷积层

卷积层中每个神经单元的**输入可以理解为过滤器的加权输出**，卷积层神经单元的输出套上激活函数就可以了。 所以关系式是下面这个样子的： [![](/static/uploads/2020/03/353dcd50b76de811896021413dccac9e.png)](/static/uploads/2020/03/353dcd50b76de811896021413dccac9e.png)

#### 池化层

池化层的输入等于输出，如果是最大池化，则使用Max求一定范围内的极值就可以了（当前例子是将 4 \* 4 的卷积最大池化为 2 \* 2 的池化层） [![](/static/uploads/2020/03/71f38ce53d679b13bd6939c56fa84745.png)](/static/uploads/2020/03/71f38ce53d679b13bd6939c56fa84745.png)

#### 输出层

而输出层和池化层的神经单元是全连接的，所以就是熟悉的 **权重 \* 池化输出求和** 公式 [![](/static/uploads/2020/03/c0c6fe84dee30c181fad010abc717b87.png)](/static/uploads/2020/03/c0c6fe84dee30c181fad010abc717b87.png)

#### 平方误差（损失函数）

平方误差依旧由 正解 - 输出神经单元的输出，做平方和 [![](/static/uploads/2020/03/fbd77c2420e048ec6f123068fbeda8d4.png)](/static/uploads/2020/03/fbd77c2420e048ec6f123068fbeda8d4.png)

### 梯度下降法

梯度下降如果有些忘了的话，链接在此：[深度学习的数学-梯度下降](https://blog.wj2015.com/2020/03/01/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%95%b0%e5%ad%a6-%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d/) 梯度就可以看作是代价函数的导数整体下降速度最快的分量，每个变量只需要顺着梯度做减法，即可以最快速度达到整体极值点的位置 [![](/static/uploads/2020/03/f32fa0ee30193687ed1af84d586a31a8.png)](/static/uploads/2020/03/f32fa0ee30193687ed1af84d586a31a8.png) 各个神经单元的的权重、偏置都是变量，为方便理解书中也把每个变量代表的含义标了一下，如下图所示： [![](/static/uploads/2020/03/bf6a45681386f7d34d8835471f2d8991.png)](/static/uploads/2020/03/bf6a45681386f7d34d8835471f2d8991.png)

#### 卷积层和输出层的神经单元误差（重点）

在全连接隐藏层反向递推的过程中，有一个神经单元误差的概念，忘了的可以看一下之前记录的博客：[深度学习的数学-神经单元误差和反向传播](https://blog.wj2015.com/2020/03/09/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%9a%84%e6%95%b0%e5%ad%a6-%e7%a5%9e%e7%bb%8f%e5%8d%95%e5%85%83%e8%af%af%e5%b7%ae%e5%92%8c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad/) 简单来说就是损失函数对神经单元的综合输入求偏导数，这个偏导数衡量这个**神经单元与理想状态（理想是达到0）的差距** 而卷积神经网络中，有卷积层和输出层两种类型的神经元，分别定义如下

> PS:池化层输入等于输出是没有额外变量的，过滤器的偏置和权重都体现在了卷积层的综合输入 z 上

[![](/static/uploads/2020/03/f74bacb9fb788391b924585cfde0bfc7.png)](/static/uploads/2020/03/f74bacb9fb788391b924585cfde0bfc7.png) 这两个神经单元误差影响整体平方误差画一个图就像下面这样： [![](/static/uploads/2020/03/3d7910813a06eea3950d147f2ad1fd6b.png)](/static/uploads/2020/03/3d7910813a06eea3950d147f2ad1fd6b.png)

#### 输出层的神经单元误差计算

在看神经单元误差对权重偏置的影响之前，先看一下怎么计算输出层的神经单元误差，其定义如下： [![](/static/uploads/2020/03/07b02d777d44baae27c4b8d5ea0941ad.png)](/static/uploads/2020/03/07b02d777d44baae27c4b8d5ea0941ad.png) 由平方误差的定义式可以知道 C 对 $a\_n^o$ 的偏导数

> PS:乘 1/2 就是为了不让偏导数前面带常数 2

[![](/static/uploads/2020/03/fbd77c2420e048ec6f123068fbeda8d4.png)](/static/uploads/2020/03/fbd77c2420e048ec6f123068fbeda8d4.png) [![](/static/uploads/2020/03/7eaedb1bfbca277fcefdabcbb1c92240.png)](/static/uploads/2020/03/7eaedb1bfbca277fcefdabcbb1c92240.png) 带入式子即可得到输出层的神经单元误差公式 [![](/static/uploads/2020/03/b100917b4c9becb5fc4b461cb3c5c61c.png)](/static/uploads/2020/03/b100917b4c9becb5fc4b461cb3c5c61c.png)

##### 输出层的神经单元与其权重偏置的关系

与之前一样，还是从输出层开始，建议看的时候结合这个式子看，很容易就看出来了 [![](/static/uploads/2020/03/c0c6fe84dee30c181fad010abc717b87.png)](/static/uploads/2020/03/c0c6fe84dee30c181fad010abc717b87.png) 同样根据单变量复合函数求偏导数的链式法则，把 $z\_1^o$ 看作 $w\_{i-j}^{Ok}$ 的函数，再根据输出层神经单元误差的定义，得到如下式子 [![](/static/uploads/2020/03/227ec607f0f312680f8dbfd6eae43879.png)](/static/uploads/2020/03/227ec607f0f312680f8dbfd6eae43879.png) 相同的思路，对输出层神经单元的偏置求偏导数，能得到如下结果 [![](/static/uploads/2020/03/55bb5a46ccbb2cfab5de7d97c739cfaa.png)](/static/uploads/2020/03/55bb5a46ccbb2cfab5de7d97c739cfaa.png)

##### 卷积层的神经单元误差与其权重偏置的关系

还是先看下上方贴出来的式子如下 [![](/static/uploads/2020/03/353dcd50b76de811896021413dccac9e.png)](/static/uploads/2020/03/353dcd50b76de811896021413dccac9e.png) 如果但看上面看不太明白，还可以按照书中的思路，先把式子都列出来找找规律 [![](/static/uploads/2020/03/c4a23138240fc9d4243426eb14b5113d.png)](/static/uploads/2020/03/c4a23138240fc9d4243426eb14b5113d.png) 里边的每一个 w 的偏导数一目了然 [![](/static/uploads/2020/03/697cb04a12a3e3bfc176e4ad9d2082d2.png)](/static/uploads/2020/03/697cb04a12a3e3bfc176e4ad9d2082d2.png) 再找规律整理一下就能得到式子 [![](/static/uploads/2020/03/805646ad7678d8715ecc36a354e541c4.png)](/static/uploads/2020/03/805646ad7678d8715ecc36a354e541c4.png) 再看一下上面的定义式，根据单变量复合函数的偏导数公式，很容易也能得出下面的结论 [![](/static/uploads/2020/03/76bf98db7d2b96d8b33c103889361042.png)](/static/uploads/2020/03/76bf98db7d2b96d8b33c103889361042.png) [![](/static/uploads/2020/03/65730357add151b21cef38e3dece7a22.png)](/static/uploads/2020/03/65730357add151b21cef38e3dece7a22.png)

### 反向递推关系式（重点）

为了减少偏导数的计算，再一次来到了紧张刺激的反向递归关系烧脑环节，书中依旧是以 6 \* 6 手写识别的神经网络为例，便于理解推导过程 三个输出神经单元的情况下，有三条路径可以影响到平方误差，把平方误差对 $z\_{ij}^{Fk}$ 的偏导数转向对输出层的三个 $z\_i^O$ 求偏导数，即可得到如下式子 [![](/static/uploads/2020/03/ade7a07de93360de41f5b3fff3e9b643.png)](/static/uploads/2020/03/ade7a07de93360de41f5b3fff3e9b643.png) 提取同类项可得： [![](/static/uploads/2020/03/deee209fd1bcaa456955a756d1b392df.png)](/static/uploads/2020/03/deee209fd1bcaa456955a756d1b392df.png) 然后把括号中的非神经单元误差的导数，根据关系式求出来（回想z = 权重 \* 输出的那个式子，所以 z 对 a 求导结果自然是权重） [![](/static/uploads/2020/03/c0c6fe84dee30c181fad010abc717b87.png)](/static/uploads/2020/03/c0c6fe84dee30c181fad010abc717b87.png) [![](/static/uploads/2020/03/625560022bc83ab7482f31c14a8322fa.png)](/static/uploads/2020/03/625560022bc83ab7482f31c14a8322fa.png) 然后计算 $a\_{ij}^{Pk}$ 对 $z\_{ij}^{Pk}$ 的导数 最大**池化层，输入等于输出**，等于卷积层对应神经单元的最大值；两者相等（激活函数 a = x），所以导数为1 [![](/static/uploads/2020/03/9b68ed26c8547ce841e9f3f7a83aa122.png)](/static/uploads/2020/03/9b68ed26c8547ce841e9f3f7a83aa122.png) [![](/static/uploads/2020/03/3ad8a5f23779a7bcc9c7b7adf4cfcc69.png)](/static/uploads/2020/03/3ad8a5f23779a7bcc9c7b7adf4cfcc69.png) 接着计算 $z\_{ij}^{Pk}$ 对 $a\_{ij}^{Fk}$ 的导数，最大时这两个数相等（**激活函数 a = x**），否则为0（可以理解为激活函数 a = 0 \* x） [![](/static/uploads/2020/03/0047fcedde75158c3056da249a8f9f73.png)](/static/uploads/2020/03/0047fcedde75158c3056da249a8f9f73.png) 最后那个 $z\_{ij}^{Fk}$ 对 $a\_{ij}^{Fk}$ 的导数**就是激活函数的导数**，带入即可得到如下式子： [![](/static/uploads/2020/03/2070693f498719a4185cefb9bf78846c.png)](/static/uploads/2020/03/2070693f498719a4185cefb9bf78846c.png) 推广一下，就可以通过下一层的神经单元误差反推上一层的神经单元误差 [![](/static/uploads/2020/03/eb06208de7bd6d1c4ee735940b138167.png)](/static/uploads/2020/03/eb06208de7bd6d1c4ee735940b138167.png) 最后只需要根据神经单元误差与权重和偏置的关系，即可算出梯度，再使用梯度下降法，逼近全局最低点即可

## 总结

首先根据卷积神经网络的概念引入了卷积神经网络中各层的数学关系式，并通过关系式求出了**输出的神经单元误差**及其**各层的神经单元误差与权重和偏置的关系**，最后用全连接神经网络类似的推导方法，推导出了反向递推关系式。